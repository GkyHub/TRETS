\section{Evalutation}\label{sec:evaluation}

In this section, we compare the performance of state-of-the-art neural network accelerator designs and try to evaluate the techniques mentiond in section~\ref{sec:software} and section~\ref{sec:hardware}. \rev{We mainly reviewed the FPGA based designs published in the top FPGA conferences (FPGA, FCCM, FPL, FPT), EDA conferences (DAC, ASPDAC, DATE, ICCAD), architecture conferences (MICRO, HPCA, ISCA, ASPLOS) since 2015. Because of the diversity in the adopted techniques, target FPGA chips, and experiments, we need to tradeoff between the fairness of comparison and the number of designs we can use. In this paper, we pick the designs with: 1) whole system implementation; 2) experiments on real NN models with reported speed, power, and energy efficiency.} 

The designs used for comparison are listed in Table~\ref{tab:hardware_list}. For data format, the "INT A/B" means that activations are A-bit fixed point data and weights are B-bit fixed point data. We also investigate the resource utilization and draw advices to both accelerator designers and FPGA manufacturers.

\input{table/hardware_list.tex}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig/efficiency.pdf}
    \caption{A comparison between different designs on a logarithm coordinate of power and performance. }
    \label{fig:efficiency}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig/resource.pdf}
    \caption{Resource utilization ratio of different accelerator designs.}
    \label{fig:resource}
\end{figure}

Each of the designs in Table~\ref{tab:hardware_list} drawn as a point in Figure~\ref{fig:efficiency}, using $log_{10}(power)$ as x coordinate and $log_{10}(speed)$ as y axis. Therefore, $y-x=log_{10}(energy\_efficiency)$. Besides the FPGA based designs, we also plot the GPU experimental results used in \cite{guo2017angel, han2017ese} as standards to measure the FPGA designs' performance.

\subsubsection*{\textbf{Bit-width Reduction}} Among all the designs, 1-2 bit based designs show outstanding speed and energy efficiency. This shows that extremely low bitwidth is a promising solution for high performance design. As introduced in section~\ref{sec:software:quant}, linear quantized 1-2 bit network models suffer from great accuracy loss, so this technique is better applied on simple tasks now. The energy efficiency is improved by about 100$\times$ comapred with 32-bit floating point designs. But there is still a great gap towards the $1000\times$ improvment estimation. This shows that memory access becomes the bottleneck for 1-2 bit designs~\cite{jiao2017accelerating,moss2017high,nakahara2017fully}, which only scales linearly with the bit-width. INT16/8, INT16 and INT8 are commonly adopted. But the difference between these designs are not obvious. This is due to the fact that current FPGAs implement wide multipliers in DSPs like $18\times 25$ or $18\times 18$. Using a less bits for computaion will not benefit the DSPs. The double MAC technique by~\cite{nguyen2017double} serves as a solution but is not adopted in the listed designs.

\subsubsection*{\textbf{Fast Convolution Algorithm}} Among all the 16-bit designs, \cite{lu2017evaluating} achieves the best energy efficiency and the highest speed with the help of the $6\times 6$ Winograd fast convolution, which is $1.7\times$ faster and $2.6\times$ energy efficient than the 16-bit design in \cite{zhang2017improving}. The design in \cite{zhang2017frequency} achieves $2\times$ speedup and $3\times$ energy efficiency compared with \cite{zhang2015optimizing} where both designs use 32-bit floating point data. Overall, the improvement does not match the estimation but can still reach 2-3$\times$.

\subsubsection*{\textbf{System Level Optimization}} The overall system optimization is not well addressed in most of the work. As this is also related to the HDL design quality, we can just roughly evaluate the effect. Here we compare three designs\cite{zhang2016caffeine, liu2016automatic, li2016high} on the same XC7VX690T platform and try to evaluate the effect. All the three designs implement 16-bit fixed-point data format except that ~\cite{liu2016automatic} uses 8-bit for weights. No fast convolution or sparsity is utilized in any of the work. Even though, \cite{li2016high} achieves $2.5\times$ the energy efficiency of \cite{liu2016automatic}. It shows that a system level optimization has a strong effect even comparable to the usage of fast convolution algorithm. 

We also investigate the resource utilization of the designs in Table~\ref{tab:hardware_list}. Three kinds of resouce are considered, DSP, BRAM, and logic. We plot the designs in Figure~\ref{fig:resource} using two of the utilization ratio as x and y coordinate. We draw the diagonal line of each figure to show the designs' preference on hardware resource. The BRAM-DSP figure shows a obvious preference of hardware on DSP over BRAM. Similar preference is also between DSP and logic. This indicates that current FPGA designs are more likely computation bounded. FPGA manufacturers targeting neural network applications can adjust the resource allocation accordingly.

\subsubsection*{\textbf{Comparision with GPU}} In general, FPGA based designs have achieved comparable energy efficiency to GPU with 10-100GOP/J. But the speed of GPUs still surpass FPGAs. Scaling up the FPGA based design is still a problem. Zhang, et al.~\cite{zhang2016energy} propose the FPGA cluster based solution using 16-bit fixed point computation. But the energy efficiency is worse than the other 16-bit fixed-point designs. 

\rev{Here we estimate the achievable speed of an ideal design. We use the 16-bit fixed point design in~\cite{lu2017evaluating} as a baseline, which is the best 16-bit fixed point design with both the highest speed and energy efficiency. 8-bit linear quantization can be adopted according to the analysis in section~\ref{sec:software:quant}, which achieves another $2\times$ energy efficiency and speedup by utilizing 1 DSP as 2 multipliers. Fast convolution and frequency optimization further improves the system by $4\times$ and $2\times$ respectively. Consider a sparse model which is similar to the one in~\cite{han2017ese} with 10\% non-zero values. We can estimate a similar $6\times$ improvement as~\cite{han2017ese}. In general about $12\times$ speedup and better energy efficiency can be achieved, which means 36TOP/s speed with about 25W. This shows that it is possible to achieve over 1 magnitude higher energy efficiency on FPGA over 32-bit floating point process on GPU.

Two problems are left. The first problem is: does all the techniques: reuse of DSP, sparsification, quantization, fast convolution can work well together? Pruning a single element in a 2D convlution kernel is of no use for fast convolution because the 2D kernel is always processed together. So each 2D kernels should be pruned as a whole


The other problem is whether the utilization ratio can be kept when the parallelism in the accelerator scales up. Both of the reference designs we used for estimation~\cite{han2017ese, lu2017evaluating} uses batch mode. Further increasing the batch size or implementing layer pipeline introduces higher CTC ratio, which means the bandwidth may become the bottleneck. Otherwise, more flexible hardware should be designed to be better fit into different layers.

On the other hand, 1/2-bit designs already shows outstanding speed and energy efficiency. But current network accuracy is still far away from real applications. 
}

