\section{Evaluation}\label{sec:evaluation}

In this section, we compare the performance of state-of-the-art neural network accelerator designs and try to evaluate the techniques mentioned in section~\ref{sec:software} and section~\ref{sec:hardware}. We mainly reviewed the FPGA-based designs published in the top FPGA conferences (FPGA, FCCM, FPL, FPT), EDA conferences (DAC, ASPDAC, DATE, ICCAD), architecture conferences (MICRO, HPCA, ISCA, ASPLOS) since 2015. Because of the diversity in the adopted techniques, target FPGA chips, and experiments, we need a trade-off between the fairness of comparison and the number of designs we can use. In this paper, we pick the designs with 1) whole system implementation; 2) experiments on real NN models with reported speed, power, and energy efficiency.

The designs used for comparison are listed in Table~\ref{tab:hardware_list}. For data format, the "INT A/B" means that activations are A-bit fixed-point data and weights are B-bit fixed-point data. We also investigate the resource utilization and draw advice to both accelerator designers and FPGA manufacturers.

\input{table/hardware_list.tex}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig/efficiency.pdf}
    \caption{A comparison between different designs on a logarithm coordinate of power and performance. }
    \label{fig:efficiency}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\columnwidth]{fig/resource.pdf}
    \caption{Resource utilization ratio of different accelerator designs.}
    \label{fig:resource}
\end{figure}

Each of the designs in Table~\ref{tab:hardware_list} drawn as a point in Figure~\ref{fig:efficiency}, using $log_{10}(power)$ as $x$ coordinate and $log_{10}(speed)$ as $y$-axis. Therefore, $y-x=log_{10}(energy\_efficiency)$. Besides the FPGA-based designs, we also plot the GPU experimental results used in \cite{guo2017angel, han2017ese} as standards to measure the FPGA designs' performance.

\subsubsection*{\textbf{Bit-width Reduction}} Among all the designs, 1-2 bit based designs~\cite{jiao2017accelerating, moss2017high, nakahara2017fully} show outstanding speed and energy efficiency. This shows that extremely low bit-width is a promising solution for high-performance design. As introduced in section~\ref{sec:software:quant}, linear quantized 1-2 bit network models suffer from great accuracy loss. Further developing related accelerator will be of little use. More efforts should be put on the models. Even trading speed with accuracy can be acceptable considering the current hardware performance.

Besides the 1/2bit designs, the rest of the designs adopts 32-bit floating-point data or linear quantization with 8 or more bits. According to the results in section~\ref{sec:software:quant}, within 1\% accuracy loss can be achieved. So we think the comparison between these designs is fair in accuracy. INT16/8 and INT16 are commonly adopted. But the difference between these designs is not obvious. This is because the underutilization of DSPs discussed in section~\ref{sec:hardware:cu:lbu}. The advantage of INT16 over FP32 is obvious except for \cite{zhang2017improving}, where the hard-core floating-point DSPs are utilized. To a certain extent, this shows the importance of fully utilizing the DSPs on-chip.

\subsubsection*{\textbf{Fast Convolution Algorithm}} Among all the 16-bit designs, \cite{lu2017evaluating} achieves the best energy efficiency and the highest speed with the help of the $6\times 6$ Winograd fast convolution, which is $1.7\times$ faster and $2.6\times$ energy efficient than the 16-bit design in \cite{zhang2017improving}. The design in \cite{zhang2017frequency} achieves $2\times$ speedup and $3\times$ energy efficiency compared with \cite{zhang2015optimizing} where both designs use 32-bit floating-point data and FPGA with 28nm technology node. \rev{Compare with the theoretical $4\times$ performance gain introduced in section~\ref{sec:hardware:cu:fcu}, there is still $1.3-1.5\times$ gap. Not all the layers can use the most optimized fast convolution method because of kernel size limitation.}

\subsubsection*{\textbf{System Level Optimization}} The overall system optimization is not well addressed in most of the work. As this is also related to the HDL design quality, we can roughly evaluate the effect. Here we compare three designs\cite{zhang2016caffeine, liu2016automatic, li2016high} on the same XC7VX690T platform and try to evaluate the effect. All the three designs implement 16-bit fixed-point data format except that ~\cite{liu2016automatic} uses 8-bit for weights. No fast convolution or sparsity is utilized in any of the work. Even though, \cite{li2016high} achieves $2.5\times$ the energy efficiency of \cite{liu2016automatic}. It shows that a system level optimization has a strong effect even comparable to the use of fast convolution algorithm. 

We also investigate the resource utilization of the designs in Table~\ref{tab:hardware_list}. Three kinds of resources (DSP, BRAM, and logic) are considered. We plot the designs in Figure~\ref{fig:resource} using two of the utilization ratio as x and y coordinate. We draw the diagonal line of each figure to show the designs' preference on hardware resource. The BRAM-DSP figure shows an obvious preference on DSP over BRAM. A similar preference appears on DSP over logic. This indicates that current FPGA designs are more likely computation bounded. FPGA manufacturers targeting neural network applications can adjust the resource allocation accordingly. Compared with that, the preference on logic and BRAM seems to be random. A possible explanation is that some of the designers use both logic and DSPs to implement high parallelism, while some prefers to use only DSPs to achieve high working frequency. 

\subsubsection*{\textbf{Comparision with GPU}} In general, FPGA-based designs have achieved comparable energy efficiency to GPU with 10-100GOP/J. But the speed of GPUs still surpasses FPGAs. Scaling up the FPGA-based design is still a problem. Zhang et al.~\cite{zhang2016energy} propose the FPGA-cluster-based solution using 16-bit fixed-point computation. But the energy efficiency is worse than the other 16-bit fixed-point designs. 

Here we estimate the achievable speed of an ideal design. We use the 16-bit fixed-point design in~\cite{lu2017evaluating} as a baseline, which is the best 16-bit fixed-point design with both the highest speed and energy efficiency. 8-bit linear quantization can be adopted according to the analysis in section~\ref{sec:software:quant}, which achieves another $2\times$ speedup and better energy efficiency by utilizing 1 DSP as 2 multipliers. The double frequency optimization further improves the system speed by $2\times$. Consider a sparse model which is similar to the one in~\cite{han2017ese} with 10\% non-zero values. We can estimate a similar $6\times$ improvement as~\cite{han2017ese}. In general about $24\times$ speedup and $12\times$ better energy efficiency can be achieved, which means 72TOP/s speed with about 50W. This shows that it is possible to achieve over $10\times$ higher energy efficiency on FPGA over 32-bit floating-point process on GPU.

The left problem is: does all the techniques: double MAC, sparsification, quantization, fast convolution, and the double frequency design work well together? Pruning a single element in a 2D convolution kernel is of no use for fast convolution because the 2D kernel is always processed as a whole. Directly pruning 2D kernels as a whole may help. But the reported accuracy of this method is lower~\cite{Mao2017Exploring} than a fine-grained pruning. The irregular data access pattern for processing sparse network and the increase in parallelism also brings challenges to the design of memory system and scheduling strategy.


