\section{Software Design: Model Compression}\label{sec:software}

As introduced in section~\ref{sec:design_method}, the design of energy efficient and high performacne neural network accelerator involves software and hardware co-design. In this section, we investigate the software level network model compression methods. Many researches on this topic have been proposed to reduce the number of weights or reduce the number of bitwidth for the neurons and weights, which helps reudce the computation and storage complexity. But these methods can also sacrifice the model accuracy. The trade-off between model compression and model accuracy loss is discussed in this section.

\subsection{Data Quantization}\label{sec:software:quant}
One of the most commonly used method for model compression is the quantization on the weights and neurons. The neurons and weights of a neural network is usually represented by floating point data in common developing frameworks. Recent work try to replace this representation with low-bit fixed-point data or even a small set of trained values. On one hand, using less bits for each neuron or weight helps reduce the bandwidth and storage requirement of the neural network processing system. On the other hand, using a simplified representation reduce the hardware cost for each operation. The benefit on hardware will be discussed in detail in section~\ref{sec:hardware}. Two kinds of quantization methods are discussed in this section: linear quantization and non-linear quantization.

\subsubsection{Linear Quantization}
Linear quantization finds the nearest fixed-point representation of each weight and neuron. The problem of this method is that the dynamic range of floating point data greatly exceeds that for fixed point data. Most of the weights and neurons will suffer from overflow or underflow. Qiu, et al.~\cite{qiu2016going} finds that the dynamic range of the weights and neurons in a single layer is much more limited and differs across different layers. Therefore they assign different fractional bit-widths to the weights and neurons in different layers. To decide the fractional bit-width of a set of data, i.e. the neurons or weights of a layer, the data distribution is first analyzed. A set of possible fractional bit-widths are chosen as candidate solutions. Then the solution with the best model performance on training data set is chosen. In~\cite{qiu2016going}, the optimized solution of a network is chosen layer by layer to avoid an exponential design space exploration. Guo, et al.~\cite{guo2017angel} further improves this method by fine tuning the model after the fraction bit-width of all the layers are fixed.

The method of choosing certain fractional bit-width equals to scale the data with a scaling factor of $2^k$. Li, et al.~\cite{li2016ternary} scales the weights with trained parameter $W^l$ for each layer and quantize the weights with 2-bit data, representing $W^l$, 0 and $-W^l$. The neurons in this work is not quantized. So the the network still implements 32-bit floating point operations. Zhou, et al.~\cite{zhou2016dorefa} further quantize the weights of a layer with only 1 bit to $\pm s$, where $s=E(|w^l|)$ is the expectation of the absolute value of the weights of this layer. Linear quantization is also applied to the neurons in this work.

\subsubsection{Non-linear Quantization}
Compared with linear quantization, non-linear quantization independently assigns values to different binary code. The translation from a non-linear quantized code to its corresponding value is thus a look-up table. This kind of methods helps further reduce the bit-width used for each neuron or weight. Chen, et al.~\cite{chen2015compressing} assign each of the weight to an item in the look-up table by a pre-defined hash function and train the values in look-up table. Han et al.~\cite{han2015deep} assigns the values in look-up table to the weights by clustering the weights of a trained model. Each look-up table value is set as the cluster center and further fine-tuned with training data set. This method is able to compress the weights of state-of-the-art CNN models to 4-bit without accuracy loss. Zhu, et al.~\cite{zhu2016trained} propose the ternary quantized network where all the weights of a layer are quantized to three values: $W^n$, 0, and $W^p$. Both the quantized value and the correspondance between weights and look-up table are trained. This method sacrifices less than $2\%$ accuracy loss on ImageNet data set on state-of-the-art network models. The weight bit-width is reduced from 32-bit to 2-bit, which means about $16\times$ model size compression.

\subsubsection{Comparison}
Experimental results of this method on VGG-16 model~\cite{simonyan2014very} are shown in Figure~\ref{} according to \cite{qiu2016going} and \cite{guo2017angel}. 16-bit fixed point data format shows a similar performance to the 32-bit floating point baseline. By introducing different fractional bit-widths for different layers, 8-bit fixed point format also introduces negligible accuracy loss. Further narrowing the bit-width to 6 causes an obvious accuracy loss.

\subsection{Weight Reduction}
Besides narrowing the bit-width of neurons and weights, another method for model compression is to reduce the number of weights. One kind of method is to approximate the weight matrix with a low rank representation. Qiu, et al.~\cite{qiu2016going} compress the weight matrix $W$ of an FC layer with singular value decomposition. An $m\times n$ weight matrix $W$ is replaced by the multiplication of two matrices $A_{m\times p}B_{p\times n}$. For a sufficiently small $p$, the total number of weights is reduced. This work compress the largest FC layer of VGG network to $36\%$ of its original size with $0.04\%$ classification accuracy degradation. Zhang, et al.~\cite{zhang2015efficient} use similar method for convolution layers and takes the effect of the following non-linear layer into the decomposition optimization process. The proposed method achieves $4\times$ speed up on state-of-the-art CNN model targeting at ImageNet, with only $0.9\%$ accuracy loss.

Pruning is another kind of method to reduce the number of weights. This kind of method directly remove the zeros in weights or remove those with small absolute values. The challenge in pruning is how to make more weights zero while keeping the model accuracy. One solution is the application of lasso object function during training. Liu, et al.~\cite{liu2015sparse} apply the spase group-lasso object function on the AlexNet~\cite{krizhevsky2012imagenet} model. $90\%$ weights are removed after training with less than $1\%$ accuracy loss. Another solution is to prune the zero weights during training. Han, et al.~\cite{han2015deep} directly removes the values in network with zero or small absolute value. The left weights are then fine-tuned are training set to recover accuracy. Experimental result on AlexNet show that $89\%$ weights can be removed while keeping the model accuracy.