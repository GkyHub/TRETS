\section{Software Design: Model Compression}\label{sec:software}

The design of energy efficient and high performacne neural network accelerator involves software and hardware co-design. In this section, we investigate the software level network model compression methods. Many researches on this topic have been proposed to reduce the number of weights or reduce the number of bitwidth for the neurons and weights, which helps reudce the computation and storage complexity. But these methods can also sacrifice the model accuracy. The trade-off between model compression and model accuracy loss is discussed in this section.

\subsection{Data Quantization}
One of the most commonly used method for model compression is the quantization on the weights and neurons. The neurons and weights of a neural network is usually represented by floating point data in common developing frameworks. Recent work try to replace this representation with low-bit fixed-point data or even a small set of trained values. On one hand, using less bits for each neuron or weight helps reduce the bandwidth and storage requirement of the neural network processing system. On the other hand, using a simplified representation reduce the hardware cost for each operation. The benefit on hardware will be discussed in detail in section~\ref{}. Two kinds of quantization methods are discussed in this section: linear quantization and non-linear quantization.

\subsubsection{Linear Quantization}
Linear quantization finds the nearest fixed-point representation of each weight and neuron. The problem of this method is that the dynamic range of floating point data greatly exceeds that for fixed point data. Most of the weights and neurons will suffer from overflow or underflow. Qiu, et al.~\cite{qiu2016going} finds that the dynamic range of the weights and neurons in a single layer is much more limited and differs across different layers. Therefore they assign different fractional bit-widths to the weights and neurons in different layers. To decide the fractional bit-width of a set of data, i.e. the neurons or weights of a layer, the data distribution is first analyzed. A set of possible fractional bit-widths are chosen as candidate solutions. Then the solution with the best model performance on training data set is chosen. In~\cite{qiu2016going}, the optimized solution of a network is chosen layer by layer to avoid an exponential design space exploration. Guo, et al.~\cite{guo2017angel} further improves this method by fine tuning the model after the fraction bit-width of all the layers are fixed.

The method of choosing certain fractional bit-width equals to scale the data with a scaling factor of $2^k$. Li, et al.~\cite{li2016ternary} scales the weights with trained parameter $W^l$ for each layer and quantize the weights with 2-bit data, representing $W^l$, 0 and $-W^l$. Zhou, et al.~\cite{zhou2016dorefa} further quantize the weights of a layer with only 1 bit to $\pm s$, where $s=E(|w^l|)$ is the expectation of the absolute value of the weights of this layer. 

\subsubsection{Non-linear Quantization}
Compared with linear quantization, non-linear quantization independently assigns values to different binary code. The translation from a non-linear quantized code to its corresponding value is thus a look-up table. This kind of methods helps further reduce the bit-width used for each neuron or weight. Chen, et al.~\cite{chen2015compressing} assign each of the weight to an item in the look-up table by a pre-defined hash function and train the values in look-up table. Han et al.~\cite{han2015deep} assigns the values in look-up table to the weights by clustering the weights of a trained model. Each look-up table value is set as the cluster center and further fine-tuned with training data set. This method is able to compress the weights of state-of-the-art CNN models to 4-bit without accuracy loss. Zhu, et al.~\cite{zhu2016trained} propose the ternary quantized network where all the weights of a layer are quantized to three values: $W^n$, 0, and $W^p$. Both the quantized value and the correspondance between weights and look-up table are trained. This method sacrifices less than $2\%$ accuracy loss on ImageNet data set on state-of-the-art network models. The weight bit-width is reduced from 32-bit to 2-bit, which means about $16\times$ model size compression.

Experimental results of this method on VGG-16 model~\cite{simonyan2014very} are shown in Figure~\ref{} according to \cite{qiu2016going} and \cite{guo2017angel}. 16-bit fixed point data format shows a similar performance to the 32-bit floating point baseline. By introducing different fractional bit-widths for different layers, 8-bit fixed point format also introduces negligible accuracy loss. Further narrowing the bit-width to 6 causes an obvious accuracy loss.

\subsection{Network Pruning}
Network pruning reduces the number of connections between input and output neurons. 