\section{Preliminary on Neural Network}\label{sec:preliminary}

In this section, we introduce the basic operations included in neural network algorithms. Neural network is a bio-inspired model, which usually includes several layers. Each layer receives input from a set of neurons and output a set of neurons. The synapses connecting input and output neurons are modeled as parameters, which is referred to as weights in this paper. In the rest part of this section, we introduce different types of layers in neural network models.

\subsection{Fully Connected Layer}

Fully conencted (FC) layer implements a connection between every input neuron and output neuron with a weight. This type of layer is adopted in both CNN and RNN. The input and output neurons of an FC layer are two vectors $\bf{x}$ and $\bf{y}$. The weights of this layer can be modeled as a matrix $W$. A bias vector $b$ is added to each of the output neuron. The computation of this layer is described as equation~\ref{eqt:fc}.
\begin{equation}\label{eqt:fc}
    {\bf{x}} = W{\bf{y}} + {\bf{b}}
\end{equation}

\subsection{Convolution Layer}
Convolution (CONV) layer is used for 2-d neuron process. This is commonly adopted in CNN for image process. The input and output neurons of this layer can be described as sets of 2-d feature maps, $F_{in}$ and $F_{out}$. Each feature map is denoted as a channel. A CONV layer implements a 2-d convolution kernel $K_{ij}$ for each input and output channel pair and a bias scalar $b_i$ for each output channel. The computation of a CONV layer with $M$ input channels and $N$ output channels can be described as equation~\ref{eqt:conv}.
\begin{equation}\label{eqt:conv}
    F_{out}(j) = \sum_{i=0}^{M-1} \text{conv2d}(F_{in}(i), K_{ij}) + b_j, \text{      } j=0,1,...,N-1
\end{equation}
There are varieties of 2-d convolutions in CONV layer. Usually standard convolution with padding is used when the kernel size is $3\times 3$. For larger kernels like $5\times 5$ and $7\times 7$, a stride larger than 1 is usually used to reduce the number of operation. Recent work is also using $1\times 1$ convolution kernels~\cite{he2016deep, iandola2016squeezenet}.

\subsection{Non-linear Layer}
Non-linear layer applies a non-linear function on each of the input neurons. Sigmoid function and tanh function are commonly adopted in early models are are still used in RNN for acoustic or speech recognition. Rectified linear unit (ReLU)~\cite{krizhevsky2012imagenet} is the adopted in many state-of-the-art models. This function maintains the positive neurons and filters negative neurons as zero. Varieties of ReLU are also used, such as PReLU and Leaky ReLU~\cite{xu2015empirical}.

\subsection{Pooling Layer}
Pooling layer is also used for 2-d neuron process like CONV layer. A pooling layer downsamples each of the input channel respectively, which helps reduce feature dimension. There are two kinds of down sampling method: average pooling and max pooling. Average pooling splits a feature map into small windows, i.e. $2\times2$ windows, and finds the average value of each window. Max pooling method finds the maximum value in each window. Common window size includes $2\times2$, stride=2 and $3\times3$, stride=2.

\subsection{Element-wise Layer}
Element-wize layer is usually used in RNN and is introduced in ResNet~\cite{he2016deep}. This layer receives two neuron vectors of the same size and applies element-wise operations on corresponding neurons of the two vectors. In ResNet, this layer is element-wise addition. For RNN, this layer can be element-wise subtraction or multiplication.

