\section{Introduction}\label{sec:introduction}

Recent research on Neural Network (NN) is showing great improvement over traditional algorithms in computer vision. Various network models, like convolutional neural network (CNN), recurrent neural network (RNN), have been proposed for image, video, and speech process. CNN~\cite{krizhevsky2012imagenet} improves the top-5 image classification accuracy on ImageNet~\cite{ILSVRC15} dataset from 73.8\% to 84.7\% and further helps improve object detection~\cite{girshick2014rich} with its outstanding ability in feature extraction. RNN~\cite{hannun2014deep} achieves state-of-the-art word error rate on speech recognition. In general, NN features a high fitting ability to a wide range of pattern recognition problems. This makes NN a promising candidate to many artificial intelligence applications.

But the computation and storage complexity of NN models are high. Current researches on NN are still increasing the size of NN models. Take CNN as an example. The largest CNN model for an $224\times224$ image classification requires upto 39 billion floating point operations (FLOP) and more than 500MB model parameters~\cite{simonyan2014very}. As the computation complexity is propotional to the input image size, processing images with higher resolutions may need more than 100 billion operations.

Therefore, choosing a proper computation platform for neural network based applications is important. A common CPU can perform 10-100G FLOP per second, and the power efficieny is usually below 1GOP/J. So CPUs are hard to meet the high performance requirements in cloud applications nor the low power requiremetns in mobile applications. In contrast, GPUs offer upto 10TOP/s peak performance and are good choices for high performance neural network applications. Development frameworks like Caffe~\cite{jia2014caffe} and Tensorflow~\cite{abadi2016tensorflow} also offer easy-to-use interfaces which makes GPU the first choice of neural network acceleration. 

Besides CPUs and GPUs, FPGAs are becoming a platform candidate to achieve energy efficient neural network processing. With a neural network oriented hardware design, FPGAs are able to implement high parallelism and make use of the properties of neural network computation to remove unecessary logic. Algorithm researches also show that a NN model is able to be simplified in a hardware friendly way while not hurting the accuracy of the model. Therefore FPGAs are possible to achieve higher energy efficieny compared with CPU and GPU. 

FPGA based accelerator designs are faced with two challenges in performance and flexibility:
\begin{itemize}
    \item Current FPGAs usually support working frequency at 100-300MHz, which is much less than CPU and GPU. The FPGA's logic overhead for reconfigurability also reduces the overall system performance. Straight forward design on FPGA is hard to achieve high performance and high energy efficiency.
    \item Implementation of neural networks on FPGAs is much harder than that on CPUs or GPUs. Development framework like Caffe and Tensorflow for CPU and GPU is needed for FPGA.
\end{itemize}
 
Many researches on the above two problems have been carried out to implement energy efficient and flexible FPGA based neural network accelerators. In this paper, we summarize the techniques proposed in these work from the following aspects:
\begin{itemize}
    \item We first give a simple model on FPGA based neural network accelerator performance to analyze the methodology in energy efficienct design.
    \item We investigate current techniques for high performance and energy efficient neural network accelerator designs. We introduce the techniques in both software and hardware level and estimate the effect of these techniques.
    \item We compare state-of-the-art neural network accelertor designs to evaluate the techniques introduced and estimate the achievable performance of FPGA based accelerator design, which is at least $40\times$ better energy efficienct than current GPUs.
    \item We investigate state-of-the-art automatic design methods of FPGA based neural network accelerators. 
\end{itemize}

The rest part of this paper is organized as follows: Section~\ref{sec:preliminary} introduces the basic operations of neural networks. Section~\ref{sec:software} and section~\ref{sec:hardware} review the techniques on neural network accelerator in software and hardware level respectively. Section~\ref{sec:evaluation} compares existing designs and evaluate the techniques. Section~\ref{sec:flexibility} introduce the methods for a flexible accelerator design. Section~\ref{sec:conclusion} concludes this paper.