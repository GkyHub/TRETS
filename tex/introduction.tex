\section{Introduction}\label{sec:introduction}

Recent research on Neural Network (NN) is showing great improvement over traditional algorithms in machine learning. Various network models, like convolutional neural network (CNN), recurrent neural network (RNN), have been proposed for image, video, and speech process. CNN~\cite{krizhevsky2012imagenet} improves the top-5 image classification accuracy on ImageNet~\cite{ILSVRC15} dataset from 73.8\% to 84.7\% in 2012 and further helps improve object detection~\cite{girshick2014rich} with its outstanding ability in feature extraction. RNN~\cite{hannun2014deep} achieves state-of-the-art word error rate on speech recognition. In general, NN features a high fitting ability to a wide range of pattern recognition problems. This ability makes NN a promising candidate for many artificial intelligence applications.

But the computation and storage complexity of NN models are high. In Table~\ref{tab:cnn_list}, we list the number of operations, number of parameters (add or multiplication), and top-1 accuracy on ImageNet dataset~\cite{ILSVRC15} of state-of-the-art CNN models. Take CNN as an example. The largest CNN model for a $224\times224$ image classification requires up to 39 billion floating point operations (FLOP) and more than 500MB model parameters~\cite{simonyan2014very}. As the computation complexity is proportional to the input image size, processing images with higher resolutions may need more than 100 billion operations. Latest work like MobileNet~\cite{Howard2017MobileNets} and ShuffleNet~\cite{zhang2017shufflenet} are trying to reduce the network size with advanced network structures, but with obvious accuracy loss. The balance between the size of NN models and accuracy is still an open question today. In some cases, the large model size hinders the application of NN, especially in power limited or latency critical scenarios.

\input{table/cnn_list.tex}

Therefore, choosing a proper computation platform for neural-network-based applications is essential. A typical CPU can perform 10-100G FLOP per second, and the power efficiency is usually below 1GOP/J. So CPUs are hard to meet the high performance requirements in cloud applications nor the low power requirements in mobile applications. In contrast, GPUs offer up to 10TOP/s peak performance and are good choices for high performance neural network applications. Development frameworks like Caffe~\cite{jia2014caffe} and Tensorflow~\cite{abadi2016tensorflow} also offer easy-to-use interfaces which makes GPU the first choice of neural network acceleration. 

Besides CPUs and GPUs, FPGAs are becoming a platform candidate to achieve energy efficient neural network processing. With a neural network oriented hardware design, FPGAs can implement high parallelism and make use of the properties of neural network computation to remove additional logic. Algorithm researches also show that an NN model can be simplified in a hardware-friendly way while not hurting the model accuracy. Therefore FPGAs are possible to achieve higher energy efficiency compared with CPU and GPU. 

FPGA-based accelerator designs are faced with two challenges in performance and flexibility:
\begin{itemize}
    \item Current FPGAs usually support working frequency at 100-300MHz, which is much less than CPU and GPU. The FPGA's logic overhead for reconfigurability also reduces the overall system performance. A straightforward design on FPGA is hard to achieve high performance and high energy efficiency.
    \item Implementation of neural networks on FPGAs is much harder than that on CPUs or GPUs. Development framework like Caffe and Tensorflow for CPU and GPU is absent for FPGA.
\end{itemize}
 
Many designs addressing the above two problems have been carried out to implement energy efficient and flexible FPGA-based neural network accelerators. In this paper, we summarize the techniques proposed in these work from the following aspects:
\begin{itemize}
    \item We first give a simple model of FPGA-based neural network accelerator performance to analyze the methodology in energy efficient design.
    \item We investigate current technologies for high performance and energy efficient neural network accelerator designs. We introduce the techniques in both software and hardware level and estimate the effect of these techniques.
    \item We compare state-of-the-art neural network accelerator designs to evaluate the techniques introduced and estimate the achievable performance of FPGA-based accelerator design, which is at least $10\times$ better energy efficient than current GPUs.
    \item We investigate state-of-the-art automatic design methods of FPGA-based neural network accelerators. 
\end{itemize}

The rest part of this paper is organized as follows: Section~\ref{sec:preliminary} introduces the basic operations of neural networks and the background of FPGA-based NN accelerator. In section~\ref{sec:design_method}, we analyze the design target of NN accelerators and corresponding methods. Section~\ref{sec:software} and section~\ref{sec:hardware} review the techniques in NN model compression and accelerator design respectively. Section~\ref{sec:evaluation} compares existing designs and evaluate the techniques. Section~\ref{sec:flexibility} introduces the methods for a flexible accelerator design. Section~\ref{sec:conclusion} concludes this paper.