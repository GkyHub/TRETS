\section{Design Methodology and Criteria}\label{sec:design_method}

Before going into the details of the techniques used for neural network accelerators, we first give an overview of the design methodology. In general, the design target of a neural network processing system includes the following two aspects: high speed (high throughput and lower latency), and high energy efficiency.

\textbf{Speed}. The throughput of an NN accelerator can be expressed by equation~\ref{eqt:throughput}. \rev{Peak performance, measured in operations (multiplication or addition) per second, is achieved when all the computation units work every clock cycle. Utilization denotes the average ratio of working cycles of the computation units. The workload measures the number of operations in the target neural network.} With a certain FPGA chip, the on-chip resource is limited. We can increase the peak performance by reducing the size of each computation unit and increasing the working frequency. Reducing the size of computation units can be achieved by simplifying the basic operations in neural network model, which may hurt the model accuracy and requires hardware-software co-design. Increasing working frequency, on the other hand is pure hardware design work. A high utilization ratio is kept by reasonable parallelism implementation and efficient memory system. Most of this part is affected by hardware design. But optimization on NN models can also reduce the storage requirment of a neural network model and benefits the memory system.

\begin{equation}\label{eqt:throughput}
    throughput = \frac{actual\_performance}{workload} = \frac{peak\_performance \times utilization}{workload}
\end{equation}

\rev{Most of the FPGA based NN accelerators compute different inputs one at a time, which means the latency of the system is the reciprocal of throughput. The concurrent process of different inputs is usually realized with batch~\cite{lu2017evaluating} or layer pipeline~\cite{li2016high}. In these cases, the concurrency equals to the batch size or the number of pipeline stages. The latency of the accelerator can be described as equation~\ref{eqt:latency}. 

\begin{equation}\label{eqt:latency}
    latency = \frac{concurrency}{throughput}
\end{equation}

So in this paper, we focus more on optimizing the throughput. As different accelerators may be evaluated on different NN models, a common criterion of speed is the $actual\_performance$, which eliminates the effect of different workload to some extent.}

\rev{\textbf{Energy Efficiency}. Energy efficiency is the actual performance within a certain power cost, i.e. the average number of operations can be executed within certain energy budget. This is usually measured in (OP/s/W) or (OP/J). Given a certain network model, the workload is fixed. Increasing the energy efficiency of a neural network accelerator means to reduce the total energy cost, $E_{total}$ to process each input as shown in equation~\ref{eqt:efficiency}. This energy cost comes from 2 parts: computation and memory access, which is shown in equation~\ref{eqt:energy}. 

\begin{equation}\label{eqt:efficiency}
    energy\_efficiency = \frac{E_{total}}{workload}
\end{equation}
    
\begin{equation}\label{eqt:energy}
    E_{total} \approx N_{op}\times E_{op} + N_{SRAM\_access}\times E_{SRAM\_access} + N_{DRAM\_access}\times E_{DRAM\_access} + E_{static}
\end{equation}
}

The first item in equation~\ref{eqt:energy} is the dynamic energy cost for computation. $N_{op}$ denotes the number of operations processed. $E_{op}$ denotes the average energy cost of each operation. Given a certain network and target FPGA platform, both $N_{op}$ and $E_{op}$ are fixed. For this part, researchers have been focusing on optimizing the NN models by quantization (narrowing the bit-width used for computation) to reduce $E_{op}$ or sparsification (setting more weights to zeros) to skip the multiplications with these zeros to reduce $N_{op}$.

\rev{The second and third item in equation~\ref{eqt:energy} is the dynamic energy cost for memory access. As shown in section~\ref{sec:preliminary:fpga}, FPGA based NN accelerator usually works with external DRAM, we separate the memory access energy into DRAM part and SRAM part. $N_{xx\_access}$ dentoes to total number of bytes to be accessed from memory. $E_{xx\_access}$ denotes the energy cost for accessing each byte. This part can also be reduced by quantization and sparsification, which helps reduce $N_{xx\_access}$. Efficient on-chip memory system and scheduling method also helps to reduce $N\_{DRAM}$. $E_{xx\_access}$ can hardly be reduced given a certain FPGA platform.

The fourth item $E_{static}$ denotes the static energy cost of the system. This can hardly be improved given the FPGA chip and the scale of the design.}

From the analysis of speed and energy, we see that neural network accelerator involves both optimization on NN models and hardware. In the following sections, we will introduce previous work in these two aspects respectively.
