\section{Design Methodology}\label{sec:design_method}

Before going into the details of the techniques used for neural network accelerators, we first give an overview of the design methodology. In general, the design target of a neural network processing system includes the following aspects: high model accuracy, high throughput, lower latency, and high energy efficiency.

\rev{A larger neural network model usually results in a higher model accuracy. This means it is possible to tradeoff between the model accuracy and the hardware performance. Neural network researchers are designing more effcient network models from AlexNet~\cite{krizhevsky2012imagenet} to ResNet~\cite{he2016deep}, SqueezeNet~\cite{iandola2016squeezenet} and MobileNet~\cite{Howard2017MobileNets}. The main differences between these networks are the size of and the connections between each layer. The basic operations are the same and hardly affect the hardware design. Other methods try to achieve the tradeoff by optimizing existing NN models. Most of these methods are hardware oriented and will be discussed in detail in section~\ref{sec:software}.}

The throughput of a neural network processing system can be expressed by equation~\ref{eqt:throughput}. With a certain FPGA chip, the on-chip resource is limited. Increasing the peak performance means to reduce the size of each computation unit and increase the working frequency. Reducing the size of computation units can be achieved by simplifying the basic operations in neural network model, which may hurt the model accuracy and requires hardware-software co-design. Increasing working frequency, on the other hand is pure hardware design work. A high utilization ratio is kept by reasonable parallelism implementation and efficient memory system. Most of this part is affected by hardware design. But model compression can also reduce the storage requirment of a neural network model and benefits the memory system.
\begin{equation}\label{eqt:throughput}
    throughput = \frac{peak\_performance \times utilization}{workload}
\end{equation}

Energy efficiency is evaluated by the number of operations (multiplication or addition in this case) executed with unit energy cost. Given a certain network model, the energy efficiency of a neural network processing system is inversely proportional to the energy cost, which is expressed in equation~\ref{eqt:energy}. The energy cost comes from 2 parts: computation and memory access. 
\begin{equation}\label{eqt:energy}
    E_{total} = N_{effect\_op}\times E_{unit\_op} + N_{mem\_access}\times E_{unit\_mem\_access}
\end{equation}

The first item in equation~\ref{eqt:energy} is the energy cost for computation. This part is greatly affected by model compression. Model compression methods can reduce the actual number of operations ($N_{effect\_op}$) to be executed on hardware and simplify the operations to reduce the unit energy cost of a single operation ($E_{unit\_op}$). Given an FPGA chip, $E_{unit\_op}$ is also affected by its hardware implementation. The second item in equation~\ref{eqt:energy} is the energy cost for memory access. The number of memory access $N_{mem\_access}$ is affected by the memory system and scheduling method. The energy for each memory access can be reduced by model compression methods by using a narrower data bit-width. 

From the analysis of throughput and energy, we see that neural network accelerator involves software-hardware co-design. In the following sections, we will introduce previous work in software and hardware level respectively.
