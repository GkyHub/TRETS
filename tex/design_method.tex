\section{Design Methodology}\label{sec:design_method}

Before going into the details of the techniques used for fast and energy efficient neural network accelerator, we first give an overview of the design methodology. In general, the design target of a neural network processing system includes the following three aspects: high model accuracy, high throughput and high energy efficiency. For certain applications, high flexibility should also be considered.

In general, a larger neural network model usually results in a higher model accuracy. Different network structures like the ones in~\cite{krizhevsky2012imagenet, simonyan2014very, he2016deep} surely affect model accuracy, but is out of the discussion of this paper. With a same model, applying model compression methods can acheive the trade-off between throughput and model accuracy. Some of the model compression methods even achieves acceleration without model accuracy loss.

The throughput of a neural network processing system can be expressed by equation~\ref{eqt:throughput}. With model compression methods, we can reduce the workload. With a certain FPGA chip, the on-chip resource is limited. Increasing the peak performance means to reduce the size of each computation unit and increase the working frequency. Reducing the size of computation units can be achieved by simplifying the basic operations in neural network model, which may hurt the model accuracy and requires hardware-software co-design. Increasing working frequency, on the other hand is pure hardware design work. A high utilization ratio is kept by reasonable parallelism implementation and efficient memory system. Most of this part is affected by hardware design. But model compression can also reduce the storage requirment of a neural network model and benefits the memory system.
\begin{equation}\label{eqt:throughput}
    throughput = \frac{peak\_performance \times utilization}{workload}
\end{equation}

Energy efficiency is evaluated by the number of operations (multiplication or addition in this case) executed with unit energy cost. Given a certain network model, the energy efficiency of a neural network processing system is inversely proportional to the energy cost, which is expressed in equation~\ref{eqt:energy}. The energy cost comes from 2 parts: computation and memory access. 
\begin{equation}\label{eqt:energy}
    E_{total} = N_{effect\_op}\times E_{unit\_op} + N_{mem\_access}\times E_{unit\_mem\_access}
\end{equation}

The first item in equation~\ref{eqt:energy} is the energy cost for computation. This part is greatly affected by model compression. Model compression methods can reduce the actual number of operations ($N_{effect\_op}$) to be executed on hardware and simplify the operations to reduce the unit energy cost of a single operation ($E_{unit\_op}$). Given an FPGA chip, $E_{unit\_op}$ is also affected by its hardware implementation. The second item in equation~\ref{eqt:energy} is the energy cost for memory access. The number of memory access $N_{mem\_access}$ is affected by the memory system and scheduling method. The energy for each memory access can be reduced by model compression methods by using a narrower data bit-width. 

From the analysis of throughput and energy, we see that neural network accelerator involves software-hardware co-design. In the following sections, we will introduce previous work in software and hardware level respectively. 
