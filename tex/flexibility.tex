\section{Design Flexibility}\label{sec:flexibility}

Though we have discussed the techniques used to achieve high speed and high energy efficiency design, most of the designs are focusing on a certain network. In this section, we discuss the methods used to adapt a neural network design to different network models and corresponding development tools. Most of the development tools implement the interface at the network model level, for example using the prototext file in Caffe~\cite{jia2014caffe} as input. Towards the hardware flexibility, mainly two kinds of methods are used: HDL model based method and instruction based method.

\subsection{HDL Model Based Method}
HDL model based method is widely adopted in FPGA based accelerators~\cite{venieris2017fpgaconvnet, morcel2017minimalist, ma2017automatic, venieris2017latency, dicecco2016caffeinated, wang2016deepburning, sharma2016high}. These proposed techniques focus on automatically generate the HDL design based on the network parameter. Difference between these methods is the selection of an intermediate level description of the network to cover the gap between high level network description and low level hardware design.

A straight forward way is no intermediate description. The design flow in \cite{ma2017automatic} search the optimized parameter for a hand coded verilog template with the input network description and platform constraint. This method is similar to the optimization methods mentioned in section~\ref{sec:hardware}. DiCecco, et al.~\cite{dicecco2016caffeinated} uses similar idea but based on OpenCL model. This enables that the development tool be integrated with Caffe and one network can be executed on different platforms. 

Venireis, et al.~\cite{venieris2017latency} describes the network model as a DFG in their design tool. Then the network computaion process is translated to hardware design with DFG mapping method.

DnnWeaver~\cite{sharma2016high} use an virtual instruction set to describe a network. The network model is first translated into instruction sequence. Then the sequence is mapped as hardware FSM states but not executed like traditional CPU instructions. 

The HDL model based methods directly modifies the hardware design to each network. This means the hardware can always achieve the best performance on the target platform. This is suitable for FPGA because of its reconfigurability. It works in situations where network switching is not frequent and the reconfiguration overhead is not cared. For example, in the large scale cloud service, the change in network models can be covered by switching between different FPGA chips. So each of the FPGA do not need to be configured frequently.

\subsection{Instruction Based Method}

Instruction based methods try to run different networks on a same hardware design. The difference between these work is the granularity of instruction. At a lower level, Guo, et al.~\cite{guo2017angel} propose the instruction set with only three kinds of instructions: LOAD, CALC, and SAVE. The granularity of the LOAD and SAVE instructions are the same as the data tiling size. Each CONV executes a set of 2-D convolutions given the feature map size encoded in the instruction. The channel number is fixed as the hardware unrolling parameter. At this level, the software compiler is able to carry out static scheduling and dynamic data reuse strategy according the certain layer. 

Zhang et al.~\cite{zhang2016caffeine} uses a layer level instruction. The control of a CNN layer is designed as a configurable hardware FSM. Compared with \cite{guo2017angel}, this reduce the memory access for instruction access while increase the hardware cost on the configurable FSM.

Instruction based methods do not modify hardware and thus enables that the accelerator can switch between networks at run time. An example of the application senario is the real-time video processing system on a mobile platform. The process of a single frame can involve different networks if the task is complex enough. Reconfigure the hardware causes unacceptable overhead while instruction based methods can solve the problem if all the instructions of all the networks are prepared in memory. 

\subsection{Mixing Method}
Wang, et al.~\cite{wang2016deepburning} propose a method mixing the above two by both optimizing hardware design and compile software instructions. The hardware is first assembled with pre-defined HDL templates using the optimized hardware parameter. The data control flow of the computation process is controlled by software binaries, which is compiled according to the network description. It is possible that the hardware can be used for a new network by simply changing the software binaries. 

