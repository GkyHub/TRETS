\begin{table}[htbp]
    \centering
    \caption{Performance and resource utilization of state-of-the-art neural network accelerator designs}
    \begin{tabular}{|r|c|c|c|c|c|c|c|c|}
        \hline
        \multicolumn{1}{|c|}{} & Data  & Perf. & Power & Efficiency & \multicolumn{3}{c|}{Resource(\%)} & \multirow{2}[4]{*}{FPGA chip} \bigstrut\\
    \cline{6-8}    \multicolumn{1}{|c|}{} & Format & (GOP/s) & (W)   & (GOP/J) & DSP   & logic & BRAM  &  \bigstrut\\
        \hline
            \cite{aydonat2017opencl} & FP16  & 1382  & 45    & 30.7  & 97    & 58    & 92    & Arria 10 GX1150 \bigstrut[t]\\ \hline
            \cite{han2017ese} & INT16/12 & 2520  & 41    & 61.5  & 54.4  & 88.6  & 87.7  & XCKU060 \\ \hline
            \cite{venieris2017fpgaconvnet} & INT16 & 12.73 & 1.75  & 7.27  & 94.54 & 66.64 & 6.07  & XC7Z020 \\ \hline
            \cite{zhang2017frequency} & FP32  & 123.5 & 13.18 & 9.37  & 87.5  & 85.4  & 64    & Stratix V \\ \hline
            \multicolumn{1}{|c|}{\multirow{2}[2]{*}{\cite{zhang2017improving}}} & INT16 & 1790  & 37.46 & 47.8  & 91    & 43    & 53    & \multirow{2}[2]{*}{GX1150}\\
            \cline{2-8}    \multicolumn{1}{|c|}{} & FP32  & 866   & 41.73 & 20.75 & 87    & -     & 46    &  \\ \hline
            \cite{ma2017optimizing} & INT16-8 & 645.25 & 21.2  & 30.43 & 100   & 38    & 70    & GX1150 \\ \hline
            \cite{qiu2016going} & INT16 & 136.97 & 9.63  & 14.22 & 89.2  & 83.5  & 86.7  & XC7Z045 \\ \hline
            \cite{suda2016throughput} & INT16-8 & 117.8 & 19.1  & 6.2   & 12.5  & 22    & 65.2  & 5SGSD8 \\ \hline
            \cite{zhang2015optimizing} & FP32  & 61.62 & 18.61 & 3.3   & 80    & 61.3  & 50    & XC7VX485T \\ \hline
            \cite{guan2017fp} & INT16 & 364.4 & 25    & 14.6  & 65    & 25    & 46    & 5SGSMD5 \\ \hline
            \cite{lu2017evaluating} & INT16 & 2940.7 & 23.6  & 124.6 &   -   &   -   &   -   & ZCU102 \\ \hline
            \cite{podili2017fast} & INT32 & 229   & 8.04  & 28.5  & 100   & 83.7  & 17.6  & Stratix V \\ \hline
            \cite{nakahara2017fully} & 1bit  & 329.47 & 2.3   & 143.2 & 0.5   & 34.4  & 11.4  & Zynq XC7Z020 \\ \hline
            \cite{jiao2017accelerating} & 2bit  & 410.22 & 2.26  & 181.51 & 40.5  & 82.7  & 37.7  & Zynq XC7Z020 \\ \hline
            \cite{moss2017high} & 1bit  & 40770 & 48    & 849.38 &   -   &   -   &   -   & GX1155 \\ \hline
            \cite{li2016high} & INT16 & 565.94 & 30.2  & 22.15 & 59.56 & 63.21 & 65.07 & XC7VX690T \\ \hline
            \cite{liu2016automatic} & INT16-8 & 222.1 & 24.8  & 8.96  & 39.9  & 26.6  & 39.7  & XC7VX690T \\ \hline
        \multicolumn{1}{|c|}{\multirow{2}[0]{*}{\cite{zhang2016energy}}} & \multirow{2}[0]{*}{INT16} & \multirow{2}[0]{*}{1280.3} & \multirow{2}[0]{*}{160} & \multirow{2}[0]{*}{8} & \multirow{2}[0]{*}{-} & \multirow{2}[0]{*}{-} & \multirow{2}[0]{*}{-} & XC7Z020+ \\
        \multicolumn{1}{|c|}{} &       &       &       &       &       &       &       & XC7VX690T$\times$6 \\ \hline
            \cite{guo2017angel} & INT8  & 84.3  & 3.5   & 24.1  & 87    & 84    & 89    & XC7Z045 \\ \hline
            \cite{xiao2017exploring} & INT16 & 229.5 & 9.4   & 24.42 & 91.9  & 71    & 83.2  & XC7Z045 \\ \hline
            \cite{guan2017fpga} & FP32  & 7.26  & 19.63 & 0.37  & 42    & 65.31 & 52.04 & XC7VX485T \\ \hline
            \cite{zhang2016caffeine} & INT16 & 354   & 26    & 13.6  & 78    & 81    & 42    & XC7VX690T \\ \hline
        \end{tabular}%
    \label{tab:hardware_list}%
  \end{table}%